# -*- coding: utf-8 -*-
"""PROJECT3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HPi5KzVoKbtUy1zlEXLGbJRPwkjX6FJT

# Supervised Learning - Foundations Project: ReCell

## Problem Statement

### Business Context

Buying and selling used phones and tablets used to be something that happened on a handful of online marketplace sites. But the used and refurbished device market has grown considerably over the past decade, and a new IDC (International Data Corporation) forecast predicts that the used phone market would be worth \\$52.7bn by 2023 with a compound annual growth rate (CAGR) of 13.6% from 2018 to 2023. This growth can be attributed to an uptick in demand for used phones and tablets that offer considerable savings compared with new models.

Refurbished and used devices continue to provide cost-effective alternatives to both consumers and businesses that are looking to save money when purchasing one. There are plenty of other benefits associated with the used device market. Used and refurbished devices can be sold with warranties and can also be insured with proof of purchase. Third-party vendors/platforms, such as Verizon, Amazon, etc., provide attractive offers to customers for refurbished devices. Maximizing the longevity of devices through second-hand trade also reduces their environmental impact and helps in recycling and reducing waste. The impact of the COVID-19 outbreak may further boost this segment as consumers cut back on discretionary spending and buy phones and tablets only for immediate needs.


### Objective

The rising potential of this comparatively under-the-radar market fuels the need for an ML-based solution to develop a dynamic pricing strategy for used and refurbished devices. ReCell, a startup aiming to tap the potential in this market, has hired you as a data scientist. They want you to analyze the data provided and build a linear regression model to predict the price of a used phone/tablet and identify factors that significantly influence it.


### Data Description

The data contains the different attributes of used/refurbished phones and tablets. The data was collected in the year 2021. The detailed data dictionary is given below.


- brand_name: Name of manufacturing brand
- os: OS on which the device runs
- screen_size: Size of the screen in cm
- 4g: Whether 4G is available or not
- 5g: Whether 5G is available or not
- main_camera_mp: Resolution of the rear camera in megapixels
- selfie_camera_mp: Resolution of the front camera in megapixels
- int_memory: Amount of internal memory (ROM) in GB
- ram: Amount of RAM in GB
- battery: Energy capacity of the device battery in mAh
- weight: Weight of the device in grams
- release_year: Year when the device model was released
- days_used: Number of days the used/refurbished device has been used
- normalized_new_price: Normalized price of a new device of the same model in euros
- normalized_used_price: Normalized price of the used/refurbished device in euros

## Importing necessary libraries
"""

#standard libraries
import pandas as pd
import numpy as np

#for visualizing data
import matplotlib.pyplot as plt
import seaborn as sns

#for randomized data splitting
from sklearn.model_selection import train_test_split

#to build linear regression_model
import statsmodels.api as sm

#to check model performance
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

"""## Loading the dataset"""

data = pd.read_csv("used_device_data.csv")

"""## Data Overview

- Observations
- Sanity checks
"""

#check the shape of the data
data.shape

#15 columns (13 possible variables influencing price, current price, and initial prial price)
#3454 rows

data.head()

data.info()

#Looks like we are missing values for main_camera_mp, selfie_camera_mp, int_memory, ram, battery, and weight
#Looks like everything is the correct datatype, although year is recognized as an integer, might want to change that to datetime format

#statistical analysis
original_data = data.describe()
original_data

#Check for duplicates
data[data.duplicated()]

#no duplicates in data

#check amount of missing data in each column
data.isnull().sum()

#check why so many missing values in main_camera_mp
data[data['main_camera_mp'].isnull()]

data[data['selfie_camera_mp'].isnull()]

data[data['int_memory'].isnull()]

data[data['ram'].isnull()]

data[data['battery'].isnull()]

data[data['weight'].isnull()]

#looks like just nan for all the missing values, so this info was just not given, but not an error

"""## Exploratory Data Analysis (EDA)

- EDA is an important part of any project involving data.
- It is important to investigate and understand the data better before building a model with it.
- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.
- A thorough analysis of the data, in addition to the questions mentioned below, should be done.

**Questions**:

1. What does the distribution of normalized used device prices look like?
2. What percentage of the used device market is dominated by Android devices?
3. The amount of RAM is important for the smooth functioning of a device. How does the amount of RAM vary with the brand?
4. A large battery often increases a device's weight, making it feel uncomfortable in the hands. How does the weight vary for phones and tablets offering large batteries (more than 4500 mAh)?
5. Bigger screens are desirable for entertainment purposes as they offer a better viewing experience. How many phones and tablets are available across different brands with a screen size larger than 6 inches?
6. A lot of devices nowadays offer great selfie cameras, allowing us to capture our favorite moments with loved ones. What is the distribution of devices offering greater than 8MP selfie cameras across brands?
7. Which attributes are highly correlated with the normalized price of a used device?
"""

#UNIVARIATE ANALYSIS:

#create barplot showing amount of phones from each brand appear in this dataset
plt.figure(figsize=(12, 5))
sns.countplot(x=data['brand_name'], order=data['brand_name'].value_counts().index)
plt.ylabel('Count')
plt.xlabel('Brand Name')
plt.title('Count of Each Brand')
plt.xticks(rotation=90)
plt.show()

#Samsung is the most common brand in this dataset. Samsung phones dominate this dataset.
#The least common brands in this dataset are Oneplus, Google, and Infinix

#create barplot showing amount of phones from each os appear in this dataset
plt.figure(figsize=(10, 5))
sns.countplot(x=data['os'], order=data['os'].value_counts().index)
plt.ylabel('Count')
plt.xlabel('Operating System')
plt.title('Count of Each Operating System')
plt.xticks(rotation=90)
plt.show()

#This dataset is mostly Android os, which is either an error in data collection, or means that resold phones are just primarily Androids.

#creating a boxplot and histogram to visualize screen sizes

plt.figure(figsize=(12, 5))
sns.boxplot(data=data, x='screen_size');
plt.show()

plt.figure(figsize=(12, 5))
sns.histplot(data=data, x='screen_size');
plt.show()

#the barplot shows the median is around 13, and the 25th percentile is not far below that; the 75th percentile is around 16
#there are many outliers in this graph on both ends, meaning the whiskers on this boxplot do not represent maximum and minimum values

#the histogram does not display a normal distribution; we see the data is concentrated at our median of around 13
#many phones have a screen size of around 13, 15, and 10 inches, so these are the more standard phone screen sizes

#create barplot number of phones with and without 5g
plt.figure(figsize=(10, 5))
sns.countplot(x=data['5g'], order=data['5g'].value_counts().index)
plt.ylabel('Count')
plt.xlabel('5g')
plt.title('Count Phones With and Without 5g')
plt.show()

#large majority of phones being resold do not have 5g

#creating a boxplot and histogram to visualize resolution of the rear camera in megapixels (main_camera_mp)

plt.figure(figsize=(12, 5))
sns.boxplot(data=data, x='main_camera_mp');
plt.show()

plt.figure(figsize=(12, 5))
sns.histplot(data=data, x='main_camera_mp');
plt.show()

#Boxplot tells us most phones being resold have a median around 8, with the left whisker reaching the minimum of 0, but there are outliers on the other end, which is not surprising

#Histogram shows us the most common resolution values (around 5, 8, and 13)

#creating a boxplot and histogram to visualize resolution of the front camera in megapixels (main_camera_mp)

plt.figure(figsize=(12, 5))
sns.boxplot(data=data, x='selfie_camera_mp');
plt.show()

plt.figure(figsize=(12, 5))
sns.histplot(data=data, x='selfie_camera_mp');
plt.show()

#Much lower reoltion than rear camera

#boxplot shows that median is 5; minimum is 0; a couple outliers with very high res on front camera

#histogram shows what the most common resolution values are (around 1, 5, and 8)

#creating a boxplot and histogram to visualize amount of internal memory (ROM) in GB (int_memory)

plt.figure(figsize=(12, 5))
sns.boxplot(data=data, x='int_memory');
plt.show()

plt.figure(figsize=(12, 5))
sns.histplot(data=data, x='int_memory');
plt.show()

#There are a couple of extreme outliers, that I will check later to see if it is an error

#Most common amount of ROM is 32 GB

#creating a boxplot and histogram to visualize amount ram

plt.figure(figsize=(12, 5))
sns.boxplot(data=data, x='ram');
plt.show()

plt.figure(figsize=(12, 5))
sns.histplot(data=data, x='ram');
plt.show()

#The boxplot shows us that there are some extreme outliers in the data, making the IQR look minuscule in comparison
#Most common amount of ram is 4 GB

#creating a boxplot and histogram to visualize battery size

plt.figure(figsize=(12, 5))
sns.boxplot(data=data, x='battery');
plt.show()

plt.figure(figsize=(12, 5))
sns.histplot(data=data, x='battery');
plt.show()

#Many outliers on right side of graph (many phones have unusually large battery)
#Median is around 3000

#creating a boxplot and histogram to visualize phone weight

plt.figure(figsize=(12, 5))
sns.boxplot(data=data, x='weight');
plt.show()

plt.figure(figsize=(12, 5))
sns.histplot(data=data, x='weight');
plt.show()

#The boxplot shows the weight has a median of around 160 grams, with outliers on both ends
#outliers on the right size are much more prominent, which is expected because people are probably mostly selling older, heavier phones
#outliers on left end might indicate a couple of newer phones becaus ethey are so much lighter

#the histogram shows a normal distribution, which is expected since this is a continuous variable
#however, the graph is right skewed and we can still see the thicker right tail, showing all the outliers

#create barplot of number of phones released from each year
plt.figure(figsize=(10, 5))
sns.countplot(x=data['release_year'], order=data['release_year'].value_counts().index)
plt.ylabel('Count')
plt.xlabel('Release Year')
plt.title('Year Released')
plt.show()

#Most phones being resold are from 2014, and the graph descends by year, which is expected because people are mostly selling older phones

#creating a boxplot and histogram to visualize days used

plt.figure(figsize=(12, 5))
sns.boxplot(data=data, x='days_used');
plt.show()

plt.figure(figsize=(12, 5))
sns.histplot(data=data, x='days_used');
plt.show()

#Median is around 700 days
#No outliers
#We see there are no unused phones being resold, which is good because this is data for selling used phones, so this would have indicated an error in the data

#The histogram loosly resembles a normal distribution; on the right side it is closer to uniform

#creating a boxplot and histogram to visualize the used price (price of the used/refurbished device in euros)

plt.figure(figsize=(12, 5))
sns.boxplot(data=data, x='normalized_used_price');
plt.show()

plt.figure(figsize=(12, 5))
sns.histplot(data=data, x='normalized_used_price');
plt.show()

#Boxplot shows a median at around 4.5 (normalized number to be compared to original price)
#Many outliers on both ends, which is not surprising since this is typically what you would see on a reselling site

#histogram shows a normal distribution of prices

#creating a boxplot and histogram to visualize the new price (price of a new device of the same model in euros)

plt.figure(figsize=(12, 5))
sns.boxplot(data=data, x='normalized_new_price');
plt.show()

plt.figure(figsize=(12, 5))
sns.histplot(data=data, x='normalized_new_price');
plt.show()

#histogram shows that the original price is not normally distributed - this is a very interesting finding since the reselling price was normally distributed
#this could be because new phones are more likely to be comprised of all low end or high end components
#however, the reselling price is effected by so many variables that are not all high or low end components since there may be certain issues with the phone, but other high end components
#or the opposite with lower quality components, but it is still in good condition

#Most of these graphs did not have a normal distribution because although they can be continous variables, there are often more common standard sizes within the range, so the data will be concentrated at those values

#BIVARIATE DATA ANALYSIS

#check correlation between each variable
num_vars = []
for column in data.columns:
  if data[column].dtype == 'float' or data[column].dtype == 'int':
    num_vars.append(column)

plt.figure(figsize=(10,10))
sns.heatmap(data[num_vars].corr(), annot=True, cmap='Spectral', vmin=-1, vmax=1);

#High positive correlations between screen size and weight, screen size and battery, battery and screen size, battery and weight, weight and screen size, normalized used price and normalized old price
#Moderate positive correlations between screen size and normalized used price, main camera resolution and normalized used price, main camera resolution and normalized new price, selfie camer resolution and release year, ram and normalized used price, ram and normalized new price, battery and normalized used price, release year and normalized used price
#High negative correlations: release year and days used
#Moderate negative correlations: days used and selfie camera

#Significant findings:
#selfie camera resolution had a much higher correlation with the used price than the new price; could indicats that this is a component that people value more now, so if a phone has bad selfie camera quality it will likely have lower value; more likely, this is a multicollinearity issue with selfie camera quality having a relationship with release year, which has a greater effect on resell price
#selfie camera resolution had a much more significant positive correlation with release year than rear cameras did, indicating that selfie camera resoltion was increasing drastically with release year after 2014, so companies were improving on selfie cameras much more than rear cameras
#battery has a much higher correlation with normalized used price than normalized new price, indicating that battery size is a component of the device that holds value over the years; larger battery=good reselling price
#release year has a 0.51 correlation with reselling price - significant because I expected it to be a stronger correlation; means this is not the sole indicator or even a significant driving indicator in reselling price

num_vars

cat_vars = []
for column in data.columns:
  if data[column].dtype != 'float' and data[column].dtype != 'int':
    cat_vars.append(column)

cat_vars

def make_bp(cat, num):
  plt.figure(figsize=(10,10))
  plt.xticks(rotation=90)
  plt.title(f"{cat} versus {num}")
  plt.xlabel(cat)
  plt.ylabel(num)
  sns.boxplot(data=data,x=cat,y=num, showmeans=True);

#create boxplot for brand name versus screen size
make_bp('brand_name','screen_size')

#Looks like Nokia has some of the smallest phones being sold by screen size; Apple has some of the largest
#However, both brands have large variance because they have so many different types of phones with different screen sizes

#create boxplot for brand name versus main camera mp
make_bp('brand_name','main_camera_mp')

#Motorola, Nokia, and ZTE have some very significant outliers with extremely high resolution

#create boxplot for brand name versus selfie camera mp
make_bp('brand_name','selfie_camera_mp')

#Less extreme outliers on this graph; shows mist companies prioritize strengthening the resolution on the rear camera than the front camera

#create boxplot for brand name versus int_memory
make_bp('brand_name','int_memory')

#Most brands have lower overall internal memory, with many high outliers

#create boxplot for brand name versus ram
make_bp('brand_name','ram')

#Oneplus' ram size is well above the overall median

#create boxplot for brand name versus main battery
make_bp('brand_name','battery')

#create boxplot for brand name versus weight
make_bp('brand_name','weight')

#Apple has created a wide range of phones of varius weights; stands out as having particularly large values in IQR
#Samsung has made many lightweight devices

#create boxplot for brand name versus realese year
make_bp('brand_name','release_year')

#This graph shows which companies only have devices from earlier dates indicating they stopped producing phones earlier on (could mean greater resell value if they are a collector's item)
#Some of the brands include Celkon and Karbonn

#create boxplot for brand name versus days used
make_bp('brand_name','days_used')

#Google, Realme, and Infinix phones were resold despite having fewer days of use, so do these brands have higher value overall?

print(f"Mean normalized price of used Google Device: {data[data['brand_name']=='Google']['normalized_used_price'].mean()}")
print(f"Mean normalized price of used Realme Device: {data[data['brand_name']=='Realme']['normalized_used_price'].mean()}")
print(f"Mean normalized price of used Infinix Device: {data[data['brand_name']=='Infinix']['normalized_used_price'].mean()}")
print(f"Mean normalized price of all used Devices: {data['normalized_used_price'].mean()}")

#They all have a slightly above average resale price

#create boxplot for brand name versus used price
make_bp('brand_name','normalized_used_price')

#Apple overall has highest resale values
#Theory that brands that stopped selling earlier would have extreme outliers because they are being sold as collector's items does not seem to be true based on this graph

#create boxplot for brand name versus used price
make_bp('brand_name','normalized_new_price')

#for the most part this graph looks pretty similar to the boxplots for used prices per brand, showing brands have similar variance in price between resale price and initial price

#create boxplot for operating system versus screen size
make_bp('os','screen_size')

#IOS tends to have larger screens

#create boxplot for operating system versus main camera resolution
make_bp('os','main_camera_mp')

#Android tends to have highest main camera resolution

#create boxplot for operating system versus selfie resolution
make_bp('os','selfie_camera_mp')

#Consistent results with previous graph - Android also tends to have hightest selfie camera resolution

#create boxplot for operating system versus memory
make_bp('os','int_memory')

#Android and IOS both have some very extreme outliers with very high internal memory
#However, internal memory is not highly coreelated with either new or resale price

#create boxplot for operating system versus ram
make_bp('os','ram')

#Android has a couple extreme outliers with very high and low ram

#create boxplot for operating system versus battery
make_bp('os','battery')

#IOS seems to have the most consitently large battery energy capacity

#create boxplot for operating system versus weight
make_bp('os','weight')

#Android, again has many outliers
#IOS has the most consistently heavy devices

#create boxplot for operating system versus ram
make_bp('os','release_year')

#Out of the devices being resold, Android and IOS are on devices from all years
#Windows and other os are mainly on older devices

#create boxplot for operating system versus days used
make_bp('os','days_used')

#Pretty similar boxplots
#Android has the most consitently resold devices that were used for fewer days

#create boxplot for operating system versus normalized used price
make_bp('os','normalized_used_price')

#Devices with Android of IOS on them are the most consistently resold devices at a higher price
#Android has many outliers on both ends

#create boxplot for operating system versus normalized new price
make_bp('os','normalized_new_price')

#Very similar graphs for new and used device prices

#Android has many outliers in the graphs for operating systems; means this operating system is used on a wide variety of devices

#create boxplot for whether or not device has 4g versus screen size
make_bp('4g','screen_size')

#Devices with 4g tend to have larger screen sizes

#create boxplot for whether or not device has 4g versus main camera resolution
make_bp('4g','main_camera_mp')

#Devices with 4g tend to have higher main camera resolution

#create boxplot for whether or not device has 4g versus selfie camera resolution
make_bp('4g','selfie_camera_mp')

#Devices with 4g tend to have higher selfie camera resolution - consistent with previous graph

#create boxplot for whether or not device has 4g versus memory
make_bp('4g','int_memory')

#Devices with 4g tend to have larger memory; however, both have extreme large outliers, so there are some devices without 4g, but with large memory

#create boxplot for whether or not device has 4g versus ram
make_bp('4g','ram')

#Both devices with and with 4g have extreme outliers on the low end of ram
#however, only devices with 4g have outliers on the higher end of ram

#create boxplot for whether or not device has 4g versus battery
make_bp('4g','battery')

#Devices with 4g tend to have larger battery
#However, devices without 4g have many high outliers, meaning they do have some devices without 4g and a large battery

#create boxplot for whether or not device has 4g versus weight
make_bp('4g','weight')

#Devices with 4g tend to be slightly heavier
#However, b oth have significant ouliers

#create boxplot for whether or not device has 4g versus release year
make_bp('4g','release_year')

#There are a couple devices released later on without 4g, but overall, later devices had 4g, and earlier devices tended to not have 4g

#create boxplot for whether or not device has 4g versus days used
make_bp('4g','days_used')

#Devices with 4g tended to have been used for more days
#Probably an indicator of multicollinearity between days used and the year the device was created

#create boxplot for whether or not device has 4g versus normalized used price
make_bp('4g','normalized_used_price')

#The resale price of devices with 4g tends to be higher

#create boxplot for whether or not device has 4g versus normalized new price
make_bp('4g','normalized_new_price')

#The new price of devices with 4g tends to be higher - consistent with previous graph

#create boxplot for whether or not device has 5g versus screen size
make_bp('5g','screen_size')

#Devices with 5g tend to have bigger screens, however many outliers with devices without 5g

#create boxplot for whether or not device has 5g versus main camera resolution
make_bp('5g','main_camera_mp')

#Devices with 5g have a higher median main camera resolution
#However, devices without 5g have a greater range and many outliers on the higher end

#create boxplot for whether or not device has 5g versus selfie camera resolution
make_bp('5g','selfie_camera_mp')

#Devices with 5g tend to have much higher main camera resolution; multicollinearity with year?

#create boxplot for whether or not device has 5g versus memory
make_bp('5g','int_memory')

#Devices with 5g tend to have a larger memory
#Howver, devices without 5g have many outliers, some even higher than devices with 5g

#create boxplot for whether or not device has 5g versus ram
make_bp('5g','ram')

#Devices with 5g tend to have higher ram
#However, many outliers in devices without 5g, some equal to outliers in devices with 5g

#create boxplot for whether or not device has 5g versus battery
make_bp('5g','battery')

#Devoces with 5g tend to have greater battery
#However, devices without 5g have a greater range and outliers with even greater battery than devices with 5g

#create boxplot for whether or not device has 5g versus weight
make_bp('5g','weight')

#Devices with 5g tend to be heavier
#However, devices without 5g have a greater range and outliers with even heavier weight than devices with 5g

#create boxplot for whether or not device has 5g versus release year
make_bp('5g','release_year')

#Devices with 5g seemed to be first released (or at least devices from this dataset) in 2019, and started becoming more prominent around 2020
#Devices withought 5g have a very large range, as they have been around much longer

#create boxplot for whether or not device has 5g versus days used
make_bp('5g','days_used')

#Devices without 5g tend to be sold after more days of use than devices with 5g
#could be a multicollinearity issue - devices with 5g were made more recently, so they do not have the ability to be used for as long

#create boxplot for whether or not device has 5g versus normalized used price
make_bp('5g','normalized_used_price')

#Devices with 5g sell for a greater price than devices without 5g
#However, devices without 5g have a large range and many outliers indicating there are some devices without 5g that still resell for a descent amount of money

#create boxplot for whether or not device has 5g versus normalized new price
make_bp('5g','normalized_new_price')

#Devices with 5g tend to have an initial price that was higher than devices without 5g
#However, devices without 5g have a large range and many outliers, indicating some devices were initially sold for almost as high a price as some devices with 5g
#Similar to previous graph

data.head()

data.describe()

#Answering questions:

#Questions:
#What does the distribution of normalized used device prices look like?
#What percentage of the used device market is dominated by Android devices?
#The amount of RAM is important for the smooth functioning of a device. How does the amount of RAM vary with the brand?
#A large battery often increases a device's weight, making it feel uncomfortable in the hands. How does the weight vary for phones and tablets offering large batteries (more than 4500 mAh)?
#Bigger screens are desirable for entertainment purposes as they offer a better viewing experience. How many phones and tablets are available across different brands with a screen size larger than 6 inches?
#A lot of devices nowadays offer great selfie cameras, allowing us to capture our favorite moments with loved ones. What is the distribution of devices offering greater than 8MP selfie cameras across brands?
#Which attributes are highly correlated with the normalized price of a used device?

#distribution of normalized used device prices
sns.histplot(data=data['normalized_used_price'])

#looks mostly normal; maybe slightly left skewed

sns.histplot(data=data['normalized_new_price']);

#In contrast, the distribution of new device prices is less normal.
#I think this is because of all the variables involved in pricing a used device. Whether it is in perfect condition, but has old components, or it is a nicer device, but has damaged parts, these are all factors that could change the price it is being sold for, making the variable more continuous

#percentage of the used device market operating on Android
((len(data[data['os']=='Android']))/data.shape[0])*100

#about 93% of this data is from Android devices - could be limitation of this analysis if data was primarily collected on Android devices
#or could just mean that majority of resold devices happen to be Androids

#RAM variance by brand
make_bp('brand_name','ram')

#Celkon, Nokia, and Infinix have significantly less RAM than other brands
#Oneplus has significantly greater RAM than other brands
#Oneplus has an outstanding ram size, although other brands have a few outliers that match up

print(f"The mean price for a used OnePlus device is: {data[data['brand_name']=='OnePlus']['normalized_used_price'].mean()}")
print(f"The mean price for a new OnePlus device is: {data[data['brand_name']=='OnePlus']['normalized_new_price'].mean()}")

print(f"The mean price for any used device is: {data['normalized_used_price'].mean()}")
print(f"The mean price for any new OnePlus device is: {data['normalized_used_price'].mean()}")

#The average price for both a new and used OnePlus device is well above average

#to see the exact median for each brand
data.groupby('brand_name')['ram'].median()

#One plus as a far greater median ram size than the other brands

#RAM variance by brand

plt.figure(figsize=(15,10))
plt.xticks(rotation=90)
sns.barplot(data, x='brand_name', y='ram')

#Different graph to visualize the difference in ram size for OnePlus devices

#How does the weight vary for phones and tablets offering large batteries (more than 4500 mAh)?

#Create scatterplot to visualize relationship between weight and battery size
sns.scatterplot(data, x='battery', y='weight');

#slight positive correlation

#create dataframe with only large batteries (more than 4500 mAh)
large_batteries = data[data['battery']>4500]

#create boxplot of weight in this dataframe
sns.boxplot(data=large_batteries['weight']);

#Check exact median of weight for devices with large batteries
large_batteries['weight'].describe()

#The median weight for devices with a large battery is 300g, with a minimum of 118g and a maximum of 855g

#create boplot of weight using all battery sizes to compare
sns.boxplot(data=data['weight']);

#all the outliers make it hard to compare
#create new graph without outliers
sns.boxplot(data=data['weight'], showfliers=False);

#check weight statistic for all devices
data['weight'].describe()

#the median weight for all devices is 160, with a minimum of 69 and a maximum of 855

#The devices with a large battery are significantly heavier
#This could increase price or decrease price since it is a better component, but a heavier device is less desirable

price_weight_battery = ['weight', 'battery','normalized_used_price', 'normalized_new_price']
sns.heatmap(data[price_weight_battery].corr(), annot=True, cmap='Spectral', vmin=-1, vmax=1);

#We see a significant correlation between battery size and weight
#However, we also see a correlation between both prices (especially used price) and battery, but a very weak correlation between weight and either price
#If higher weight was not an indicator of larger battery, it would probably be a negative correlation with price, but because of the relationship between weight and battery size, the correlation between weight and price is affected
#Might be multicollinearity problem

#How many phones and tablets are available across different brands with a screen size larger than 6 inches

#first look at the amount and percentage of all devices fit this criteria
print(f"{data[data['screen_size']>6].shape[0]} devices are larger than 6 inches")
print(f"{((data[data['screen_size']>6].shape[0])/data.shape[0])*100}% of all devices are larger than 6 inches")

#check how many phones with screen>6 for each brand
large_phones = data[data['screen_size']>6]
large_phones.groupby('brand_name').size()

#Visualize amount of devices available across different brands with a screen size larger than 6 inches

plt.figure(figsize=(10,5))
sns.barplot(large_phones.groupby('brand_name').size())
plt.xticks(rotation=90);
plt.ylabel('Screens>6')
plt.title('Number of Phones with Screens Larger Than 6 Inches Per Brand');

#Samsung has a very large amount of devices with a screen over 6 inches

#distribution of devices offering greater than 8MP selfie cameras across brands

#create dataframe with only devices with good selfie cameras
good_selfie = data[data['selfie_camera_mp']>8]

#create boxplot of the selfie camera mp for each brand for devices with a good selfie camera
plt.figure(figsize=(10,5))
sns.boxplot(data=good_selfie, x='brand_name', y='selfie_camera_mp');
plt.xticks(rotation=90)
plt.show()

#out of all devices with a selfie camera greater than 8MP, Oppo, Samsung, Lenovo, and Honor had consistently good quality selfie cameras

#Which attributes are highly correlated with the normalized price of a used device?

#look at heatmap again to answer this question

plt.figure(figsize=(10,5))
sns.heatmap(data[num_vars].corr(), annot=True, cmap='Spectral', vmin=-1, vmax=1)
plt.show()

#screen size, main camera mp, selfie camera mp, ram, battery, release year, and normalized new price are all at least mildly correlated.
#I believe there is multicollinearity present in this dataset that is affecting these relationships

"""## Data Preprocessing

- Missing value treatment
- Feature engineering (if needed)
- Outlier detection and treatment (if needed)
- Preparing data for modeling
- Any other preprocessing steps (if needed)
"""

#Check for missing values

data.isnull().sum()

#columns with missing values: main camera mp, selfie camera mp, int memory, ram, battery, and weight

#main camera mp has many missing variables, so I want to replace these value with different statistic from the column

#to see which statistic, I need to look at the distribution of this column again
sns.histplot(data['main_camera_mp']);

#This is not a normal distribution

#Now I am going to make a boxplot for this column to see if it has any extreme outliers
sns.boxplot(data['main_camera_mp']);

#since there are outliers present in this column (and the other columns, which I checked earlier), I am going to replace missing values with the median

#Fill all nan values in main camera mp column with the median
data['main_camera_mp'].fillna(data['main_camera_mp'].median(), inplace=True)

#Check for missing values again

data.isnull().sum()

#Got rid of all missing values in that column. Now do this with other columns with missing values

#For selfie camera mp
data['selfie_camera_mp'].fillna(data['selfie_camera_mp'].median(), inplace=True)

#for int memory
data['int_memory'].fillna(data['int_memory'].median(), inplace=True)

#for ram
data['ram'].fillna(data['ram'].median(), inplace=True)

#for battery
data['battery'].fillna(data['battery'].median(), inplace=True)

#for weight
data['weight'].fillna(data['weight'].median(), inplace=True)

#Check for missing values again

data.isnull().sum()

#no more missing values!

#Make sure no rows were lost
data.shape

#We still have 3454

#Check to see if we need to do feature Engineering

data.head()

#I could replace release year with the amount of years since the data was released to make it a more quantifiable number that can better be used in data analysis than year

#Make a new column and subract release year from 2021 (year data was released) to indicate number of years the device has been out
data['years_out'] = 2021 - data['release_year']

data.head()

#Now we can drop the date column
data.drop('release_year', axis=1, inplace=True)

data.head()

# outlier detection using boxplot
num_cols = data.select_dtypes(include=np.number).columns.tolist()

plt.figure(figsize=(15, 10))

for i, variable in enumerate(num_cols):
    plt.subplot(3, 4, i + 1)
    sns.boxplot(data=data, x=variable)
    plt.tight_layout(pad=2)

plt.show()

#there are many outliers in the data, but I am going to keep them because they are real datapoints, and in this context, I think it makes sense

#Preparing data for modeling

# defining X and y variables
X = data.drop(["normalized_used_price"], axis=1)
y = data["normalized_used_price"]

print(X.head())
print(y.head())

#add the intercept to data
X = sm.add_constant(X)

#creating dummy variables
X = pd.get_dummies(
    X,
    columns=X.select_dtypes(include=["object", "category"]).columns.tolist(),
    drop_first=True
)
X.head()

#converting everything to float type for modeling
X = X.astype(float)
X.head()

#splitting the data into train and test data

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

print("Number of rows in train data =", x_train.shape[0])
print("Number of rows in test data =", x_test.shape[0])

"""## EDA

- It is a good idea to explore the data once again after manipulating it.
"""

data.columns

#compare boxplots to original data statistics:
original_data

#current data statistics
data.describe()

#All the median, 25th percentile, and 7th percentile stayed the same
#The means did not change too drastically which is very good because this could have skewed the data
#particularly, main camera mp kept a very similar mean which is very good because we imputed the median on a lot of rows for that column

#I am going to check the columns that were changed due to missing values (main_camera_mp, selfie_camera_mp, int_memory, ram, battery, and weight)

#boxplot for main camera mp
sns.boxplot(data=data, x='main_camera_mp');

#This boxplot is still pretty similar to what it originally was (it originally has a median of 8, 25th percentile was 5, 75th percentile was 13)
#This is very good since the majority of the replaced data was from this column

#boxplot for selfie_camera_mp
sns.boxplot(data=data, x='selfie_camera_mp');

#very similar to original graph

#boxplot for int_memory
sns.boxplot(data=data, x='int_memory');

#very similar to original graph

#boxplot for ram
sns.boxplot(data=data, x='ram');

#very similar to original graph

#boxplot for battery
sns.boxplot(data=data, x='battery');

#very similar to original graph

#boxplot for weight
sns.boxplot(data=data, x='weight');

#very similar to original graph

"""## Model Building - Linear Regression"""

#look at original model performance
olsmodel = sm.OLS(y_train, x_train).fit()
print(olsmodel.summary())

#This r squared means that the variables included in our model can explain 84.5% of the variance in the training set. This is very good.
#looks like the new price has the largest affect in predicting the used price (this is expected)

"""## Model Performance Check"""

#same functions as all the previous practice sets

#function to compute adjusted R-squared

def adj_r2_score(predictors, targets, predictions):
    r2 = r2_score(targets, predictions)
    n = predictors.shape[0]
    k = predictors.shape[1]
    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))


#function to compute MAPE
def mape_score(targets, predictions):
    return np.mean(np.abs(targets - predictions) / targets) * 100


#function to compute different metrics to check performance of a regression model
def model_performance_regression(model, predictors, target):
    pred = model.predict(predictors)

    r2 = r2_score(target, pred)
    adjr2 = adj_r2_score(predictors, target, pred)
    rmse = np.sqrt(mean_squared_error(target, pred))
    mae = mean_absolute_error(target, pred)
    mape = mape_score(target, pred)

    #creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "RMSE": rmse,
            "MAE": mae,
            "R-squared": r2,
            "Adj. R-squared": adjr2,
            "MAPE": mape,
        },
        index=[0],
    )

    return df_perf

#checking model performance on train set
print("Training Performance\n")
olsmodel_train_perf = model_performance_regression(olsmodel, x_train, y_train)
olsmodel_train_perf

# checking model performance on test set
print("Test Performance\n")
olsmodel_test_perf = model_performance_regression(olsmodel, x_test, y_test)
olsmodel_test_perf

#the training r2 is 0.841723, compared to our original r2 of 0.845
#this is very good because it means our model is not underfitting
#also, the RMSE and MAPE are very similar for testing and training data, so no overfitting as well; both slightly bigger for testing data, but that is expected, and it's a very small difference
#MAE suggests that the model can predict used price within a mean error of 0.184064 on the test data
#MAPE of 4.488006 on the test data means that we are able to predict within 4.488% of the used prices. This is good

"""## Checking Linear Regression Assumptions

- In order to make statistical inferences from a linear regression model, it is important to ensure that the assumptions of linear regression are satisfied.
"""

#We will be checking the following Linear Regression assumptions:
#No Multicollinearity
#Linearity of variables
#Independence of error terms
#Normality of error terms
#No Heteroscedasticity

#Test for multicollinearity

#check VIF score

from statsmodels.stats.outliers_influence import variance_inflation_factor


vif_series1 = pd.Series(
    [variance_inflation_factor(x_train.values, i) for i in range(x_train.shape[1])],
    index=x_train.columns,
)
print("VIF values: \n\n{}\n".format(vif_series1))

#There are 2 columns with high multicollinearity
#I want to eventually drop all columns with VIF>5
#columns include screen size and weight (we can ignore high VIF for dummy variables)
#However, I need to drop these columns one by one and check the VIF in between drops because dropping just one column could change the VIF scores of the others
#remember 0.845 is our initial measure of success (r sq)

#drop screen size first because it had the highest VIF

#find r sq for dataset without displacement
x_train2 = x_train.drop(["screen_size"], axis=1)
olsmod_1 = sm.OLS(y_train, x_train2)
olsres_1 = olsmod_1.fit()
print(
    "R-squared:",
    np.round(olsres_1.rsquared, 3),
    "\nAdjusted R-squared:",
    np.round(olsres_1.rsquared_adj, 3),
)

#after dropping screen size, r2 only decreased by 0.003
#this shows that the absence of this variable did not impact the regression much
#so data from this variable is sufficiently brought in from other variables

#same thing for weight
x_train3 = x_train.drop(["weight"], axis=1)
olsmod_2 = sm.OLS(y_train, x_train3)
olsres_2 = olsmod_2.fit()
print(
    "R-squared:",
    np.round(olsres_2.rsquared, 3),
    "\nAdjusted R-squared:",
    np.round(olsres_2.rsquared_adj, 3),
)

#after dropping weight, r2 only decreased by 0.004
#this shows that the absence of this variable did not impact the regression much
#so data from this variable is sufficiently brought in from other variables

#since screen size changed the least, this means it did not hold much value in our dataset, so we will drop this column first

#drop screen size in original data and redo regression
x_train = x_train.drop(["screen_size"], axis=1)

olsmod_5 = sm.OLS(y_train, x_train)
olsres_5 = olsmod_5.fit()
print(olsres_5.summary())

#we have to check for multicollinearity completely again to see if absence of this var changed which vars have multicollinearity issues

vif_series2 = pd.Series(
    [variance_inflation_factor(x_train.values, i) for i in range(x_train.shape[1])],
    index=x_train.columns,
)
print("VIF values: \n\n{}\n".format(vif_series2))

#we see that dropping screen size decreased VIF for weight, so we do not need to drop any more columns
#VIF for all the features is <5. No more multicollinearity

#Now we can remove the non-significant predictor variables

olsmod_9 = sm.OLS(y_train, x_train)
olsres_9 = olsmod_9.fit()
print(olsres_9.summary())

#after dropping a variable, r sq is still very close to what it originally was
#However, we still see large (greater than 0.05) p-vals (not sufficient evidence acceleration should not belong in equation)
#columns include int memory, battery, and days used (dummy vars are not a concern)
#This high p-value means that these variables are not significant in predicting used price. Not sufficient evidence they should belong in dataset

#since there are so many variables with large p-value, I used loop to get rid of all p-values>0.5, and check the regression btween drops

#initial list of columns
predictors = x_train.copy()
cols = predictors.columns.tolist()

# setting an initial max p-value
max_p_value = 1

while len(cols) > 0:
    # defining the train set
    x_train_aux = predictors[cols]

    # fitting the model
    model = sm.OLS(y_train, x_train_aux).fit()

    # getting the p-values and the maximum p-value
    p_values = model.pvalues
    max_p_value = max(p_values)

    # name of the variable with maximum p-value
    feature_with_p_max = p_values.idxmax()

    if max_p_value > 0.05:
        cols.remove(feature_with_p_max)
    else:
        break

selected_features = cols
print(selected_features)

x_train3 = x_train[selected_features]
x_test3 = x_test[selected_features]

olsmod10 = sm.OLS(y_train, x_train3).fit()
print(olsmod10.summary())

#after removing all those variables, the r2 is phenominally close to what it originally was - good

#No more large p-values
#This means that main camera mp, selfie camera mp, ram, weight, normalized price, years out, some brand names, some os, and 4g can explain 0.839 percent of the variance in used price

#Now we address other assumptions

df_pred = pd.DataFrame()

df_pred["Actual Values"] = y_train.values.flatten()  # actual values of mpg
#y_train.values converts the pandas Series or DataFrame y_train to a NumPy array.
#.flatten() converts this array into a one-dimensional array.
df_pred["Fitted Values"] = olsmod10.fittedvalues.values  # predicted values of mpg based on vars in regression analysis
#olsres_10.fittedvalues contains the fitted (predicted) values from the regression analysis
#.values converts this to a NumPy array.
df_pred["Residuals"] = olsmod10.resid.values  # residuals (diff bw actual and predicted vals)


df_pred.head()

#Test for Linearity and Independence

#LINEARITY
#linearity checks if data came from a linear model
#(if we fit our model to a linear model, what is left out should have no pattern)

#INDEPENDENCE
#residuals should be independent from eachother
#one residual should not predict next

#plot residuals and visually inspect

#plot the fitted values vs residuals
sns.set_style("whitegrid")
sns.residplot(
    data=df_pred, x="Fitted Values", y="Residuals", color="purple", lowess=True
)
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.title("Fitted vs Residual plot")
plt.show()

#mostly no pattern, which is good
#More concentrated near center of graph than sides (maybe slight non-linearity)

#Let's see where the non-linearity is coming from

# checking the distribution of variables in training set with dependent variable
sns.pairplot(data[['normalized_used_price', 'main_camera_mp', 'selfie_camera_mp', 'ram', 'weight',
       'normalized_new_price', 'years_out','4g', '5g']])
plt.show()

#normalized new price and normalized old price have a very clear relationship, which is probably causing majority of the non-linearity
#weight has a slight relationship with normalized new price and normalized used price

#Let's see how dropping normalized new price would affect our regression

#dropping normalized new price
x_train4 = x_train3.drop(["normalized_new_price"], axis=1)
olsmod_20 = sm.OLS(y_train, x_train4)
olsres_20 = olsmod_20.fit()
print(olsres_20.summary())

#dropping normalized new price drops r2 drastically, so we need to keep this variable
#we can use normalized_new_price squared as a new variable, and use that instead to eliminate non-linearity
#we might see that these are too diff, but I will a do regression analysis and check p-vals to see

# using square transformation
x_train3["normalized_new_price_sq"] = np.square(x_train3["normalized_new_price"])

# let's create a model with the transformed data
olsmod_16 = sm.OLS(y_train, x_train3)
olsres_16 = olsmod_16.fit()
print(olsres_16.summary())

#r2 increased - good
#p-vale for normalized_new_price_sq is low - good
#However, some p-values increased, so we should drop those columns now

#using the same function as before to get rid of columns with high p-values, while checking regression analysis in between

#initial list of columns
predictors = x_train3.copy()
cols = predictors.columns.tolist()

# setting an initial max p-value
max_p_value = 1

while len(cols) > 0:
    # defining the train set
    x_train_aux = predictors[cols]

    # fitting the model
    model = sm.OLS(y_train, x_train_aux).fit()

    # getting the p-values and the maximum p-value
    p_values = model.pvalues
    max_p_value = max(p_values)

    # name of the variable with maximum p-value
    feature_with_p_max = p_values.idxmax()

    if max_p_value > 0.05:
        cols.remove(feature_with_p_max)
    else:
        break

selected_features = cols
print(selected_features)

#add new column to test data
x_test["normalized_new_price_sq"] = np.square(x_test["normalized_new_price"])

x_train30 = x_train3[selected_features]
x_test30 = x_test[selected_features]

olsmod30 = sm.OLS(y_train, x_train30).fit()
print(olsmod30.summary())

#after getting rid of variables with high p-vale, r2 barely changed - good
#this means the remaining 12 variables explain 84.4% of the variance in used device prices

#Now check linearity again

#redo residuals

#recreate the dataframe with actual, fitted and residual values
df_pred2 = pd.DataFrame()

df_pred2["Actual Values"] = y_train.values.flatten()  # actual values
df_pred2["Fitted Values"] = olsmod30.fittedvalues.values  # predicted values
df_pred2["Residuals"] = olsmod30.resid.values  # residuals

df_pred.head()

#plot the fitted values vs residuals
sns.set_style("whitegrid")
sns.residplot(
    data=df_pred2, x="Fitted Values", y="Residuals", color="purple", lowess=True
)
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.title("Fitted vs Residual plot")
plt.show()

#might be slightly more random

#test for normality

#see if residuals are normally distributed

sns.histplot(df_pred2["Residuals"], kde=True)
plt.title("Normality of residuals")
plt.show()

#pretty normal - slightly left skewed

#I am not sure if this is normal enoug, so I will use a QQ plot to compare the normality of this curve against a perfectly normal distribution

import pylab
import scipy.stats as stats

stats.probplot(df_pred2["Residuals"], dist="norm", plot=pylab)
plt.show()

#this looks pretty close to normal
#I am going to accept this as sufficient evidence that the residuals follow a normal distribution

#test for homoscedasticity

#see if the variance of the residuals is stable
#we can check this just by looking at the graphs above - it does look to have stable variance, but I will use a hypothesis test to make sure

#import necessary libraries for hypothesis test
import statsmodels.stats.api as sms
from statsmodels.compat import lzip

name = ["F statistic", "p-value"]
test = sms.het_goldfeldquandt(df_pred2["Residuals"], x_train30)
lzip(name, test)

#the probability that what we observed could have happened if the null is true (it is homoscedatic) is 49%
#not enough evidence against the null - data is homoscedatic

"""## Final Model"""

#all the assumptions are now met
#check the summary of our final model

print(olsmod30.summary())

#equation of the linear regression
Equation = "normalized used price ="
print(Equation, end=" ")
for i in range(len(x_train30.columns)):
    if i == 0:
        print(olsmod30.params[i], "+", end=" ")
    elif i != len(x_train30.columns) - 1:
        print(
            olsmod30.params[i],
            "* (",
            x_train30.columns[i],
            ")",
            "+",
            end="  ",
        )
    else:
        print(olsmod30.params[i], "* (", x_train30.columns[i], ")")

x_train30.columns

x_test.columns

# dropping columns from the test data that are not in the training data
x_test2 = x_test.drop(
    ['screen_size',
       'int_memory', 'battery', 'days_used', 'brand_name_Alcatel',
       'brand_name_Apple', 'brand_name_Asus', 'brand_name_BlackBerry',
       'brand_name_Celkon', 'brand_name_Coolpad', 'brand_name_Gionee',
       'brand_name_Google', 'brand_name_HTC', 'brand_name_Honor',
       'brand_name_Huawei', 'brand_name_Infinix', 'brand_name_Karbonn',
       'brand_name_Lava', 'brand_name_Lenovo',
       'brand_name_Meizu', 'brand_name_Micromax', 'brand_name_Microsoft',
       'brand_name_Motorola', 'brand_name_Nokia', 'brand_name_OnePlus',
       'brand_name_Oppo', 'brand_name_Panasonic',
       'brand_name_Realme',
       'brand_name_Spice', 'brand_name_Vivo', 'brand_name_XOLO',
       'brand_name_ZTE', 'os_Others', 'os_Windows',
       'os_iOS', '4g_yes', '5g_yes'], axis=1
)

x_test2.columns

x_test2

#transforming the new price column in the test data corresponding to the training set
x_test2["normalized_new_price_sq"] = np.square(x_test2["normalized_new_price"])

#make predictions on the test set
y_pred = olsmod30.predict(x_test2)

#check the RMSE on the train data
rmse1 = np.sqrt(mean_squared_error(y_train, df_pred["Fitted Values"]))
rmse1

#check the RMSE on the test data
rmse2 = np.sqrt(mean_squared_error(y_test, y_pred))
rmse2

#this is very good - we want the measure of success to be very close for the training sample and the testing sample
#suggests there is not much overfitting

#check the MAE on the train data
mae1 = mean_absolute_error(y_train, df_pred["Fitted Values"])
mae1

#check the MAE on the test data
mae2 = mean_absolute_error(y_test, y_pred)
mae2

#mae is very close for test and traing data. Good
#suggests that our current model is able to predict used price within a mean error of 0.18 units on the test data.

#Now we know this model is good for prediction and inference

"""## Actionable Insights and Recommendations

- Based on this model, we are able to predict the price of a used device through this equation: normalized used price = -0.41727730370151694 + 0.0196548933373735 * ( main_camera_mp ) +  0.013283854744645426 * ( selfie_camera_mp ) +  0.023546993773401727 * ( ram ) +  0.0015971302387318648 * ( weight ) +  1.2082111403732825 * ( normalized_new_price ) +  -0.03858244293186329 * ( years_out ) +  -0.04183447812024251 * ( brand_name_LG ) +  -0.036832609866963295 * ( brand_name_Others ) +  -0.04721591042782653 * ( brand_name_Samsung ) +  -0.06273438914126221 * ( brand_name_Sony ) +  0.07454635421813148 * ( brand_name_Xiaomi ) +  -0.07154813472250998 * ( normalized_new_price_sq )
- We can explain about 84.4% of the variance in the price of used devices through these variables: main camera resolution, selfie camera resolution, ram, weight, normalized new price, the number of years the device has been out, whether it is an LG device, a device that does not fall into one of the main brand categories, a Samsung device, a Sony device, or a Xiaomi device.
- Based on these observations, I would advice the startup ReCell to use this equation to figure out which combinations of variables produce th highest resell price, and aim towards selling those devices on their site, as I assume they get a percentage of the profits. So, by using this model they can determine which kinds of devices would produce the highest profits for their company.
- Also, even if ReCell decides to sell all deices, regardless of the predicted price, being able to use this model to accurately predict the price that a customer is willing to buy it would be extremely beneficial to the company. It would bring customers in because they would be compelled by the prices on the site since they would be pricing it based soley on the components and data of what customers are willing to buy each device for.
- I would also reccommend that ReCell does not focus too much on individual characteristics of the device alone, as we see in the data that the price can vary drastically for each device depending on the combination of each type of component, not just one component alone. For example, we see that just because a device is heavy does not mean the price or value of the device decreases because this could indicate that the battery size is actually much larger, which is an attractive feature in the device, and could make the price increase significantly. This is why paying attention to the relationships between each variable, and accounting for collinearity in the data is so important.

___
"""

!jupyter nbconvert PROJECT3.ipynb --to html

