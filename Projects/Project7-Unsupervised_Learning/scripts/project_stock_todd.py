# -*- coding: utf-8 -*-
"""PROJECT_stock_Todd.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/158JE2GKNRVC3zSYtC_uYLOLp9Rqo0WmZ

# Unsupervised Learning: Trade&Ahead

**Marks: 60**

### Context

The stock market has consistently proven to be a good place to invest in and save for the future. There are a lot of compelling reasons to invest in stocks. It can help in fighting inflation, create wealth, and also provides some tax benefits. Good steady returns on investments over a long period of time can also grow a lot more than seems possible. Also, thanks to the power of compound interest, the earlier one starts investing, the larger the corpus one can have for retirement. Overall, investing in stocks can help meet life's financial aspirations.

It is important to maintain a diversified portfolio when investing in stocks in order to maximise earnings under any market condition. Having a diversified portfolio tends to yield higher returns and face lower risk by tempering potential losses when the market is down. It is often easy to get lost in a sea of financial metrics to analyze while determining the worth of a stock, and doing the same for a multitude of stocks to identify the right picks for an individual can be a tedious task. By doing a cluster analysis, one can identify stocks that exhibit similar characteristics and ones which exhibit minimum correlation. This will help investors better analyze stocks across different market segments and help protect against risks that could make the portfolio vulnerable to losses.


### Objective

Trade&Ahead is a financial consultancy firm who provide their customers with personalized investment strategies. They have hired you as a Data Scientist and provided you with data comprising stock price and some financial indicators for a few companies listed under the New York Stock Exchange. They have assigned you the tasks of analyzing the data, grouping the stocks based on the attributes provided, and sharing insights about the characteristics of each group.

### Data Dictionary

- Ticker Symbol: An abbreviation used to uniquely identify publicly traded shares of a particular stock on a particular stock market
- Company: Name of the company
- GICS Sector: The specific economic sector assigned to a company by the Global Industry Classification Standard (GICS) that best defines its business operations
- GICS Sub Industry: The specific sub-industry group assigned to a company by the Global Industry Classification Standard (GICS) that best defines its business operations
- Current Price: Current stock price in dollars
- Price Change: Percentage change in the stock price in 13 weeks
- Volatility: Standard deviation of the stock price over the past 13 weeks
- ROE: A measure of financial performance calculated by dividing net income by shareholders' equity (shareholders' equity is equal to a company's assets minus its debt)
- Cash Ratio: The ratio of a  company's total reserves of cash and cash equivalents to its total current liabilities
- Net Cash Flow: The difference between a company's cash inflows and outflows (in dollars)
- Net Income: Revenues minus expenses, interest, and taxes (in dollars)
- Earnings Per Share: Company's net profit divided by the number of common shares it has outstanding (in dollars)
- Estimated Shares Outstanding: Company's stock currently held by all its shareholders
- P/E Ratio: Ratio of the company's current stock price to the earnings per share
- P/B Ratio: Ratio of the company's stock price per share by its book value per share (book value of a company is the net difference between that company's total assets and total liabilities)

## Importing necessary libraries and data
"""

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_theme(style='darkgrid')

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# to scale the data using z-score
from sklearn.preprocessing import StandardScaler

# to compute distances
from scipy.spatial.distance import cdist, pdist

# to perform k-means clustering and compute silhouette scores
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# to visualize the elbow curve and silhouette scores
from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer

# to perform hierarchical clustering, compute cophenetic correlation, and create dendrograms
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage, cophenet

# to suppress warnings
import warnings
warnings.filterwarnings("ignore")

"""## Data Overview

- Observations
- Sanity checks
"""

#load dataset
df = pd.read_csv('stock_data.csv')

# creating a copy of each dataset
data = df.copy()

# check first 5 rows of dataset
data.head()

# last 5 rows of dataset
data.tail()

"""There are a couple columns with words that I will probably end up converting to categories (if they are not already), but most columns consist of numerical data which is good because it is easy to work with. I will check the datatypes for each and make sure correct columns are cat, and others are int."""

# check rows and columns
data.shape

"""There are 340 datapoints in our dataset, and 15 different columns/variables."""

# check var datatypes
data.info()

"""Looks like there is no missing data. All numerical data is either int or float, which works. All other variables are object, which I may convert to cat."""

# check data stats
data.describe()

"""- Current price has a large range from about 4 to about 1275. This column will most likley be a heavy predictor for which group a datapoint falls into.
- Net cash flow and net income consists of very large numbers; whole column is converted to scientific notation.
"""

#check for duplicates
data.duplicated().sum()

"""No duplicated rows."""

# ensure no missing vals
data.isna().sum()

"""No missing values.

## Exploratory Data Analysis (EDA)

- EDA is an important part of any project involving data.
- It is important to investigate and understand the data better before building a model with it.
- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.
- A thorough analysis of the data, in addition to the questions mentioned below, should be done.

**Questions**:

1. What does the distribution of stock prices look like?
2. The stocks of which economic sector have seen the maximum price increase on average?
3. How are the different variables correlated with each other?
4. Cash ratio provides a measure of a company's ability to cover its short-term obligations using only cash and cash equivalents. How does the average cash ratio vary across economic sectors?
5. P/E ratios can help determine the relative value of a company's shares as they signify the amount of money an investor is willing to invest in a single share of a company per dollar of its earnings. How does the P/E ratio vary, on average, across economic sectors?

I will plot all the object columns to better understand those vars. Then, I will use a function to plot a boxplot and histogram for all the numeric columns. Then, I will answer the guided questions above.
"""

data.head()

# we do not need to plot ticker symbol because it is a unique identification code for each datapoint.
# We will probably delete this column from the dataset because it will not provide any information.
# check how many unique vals in 'Ticker Symbol'
data['Ticker Symbol'].nunique()

# According to the variable descriptions, it looks like 'Security' is supposed to be compay name
# I think each name is unique as well, and if that is the case, we cannot plot this, and will probably drop this col too.
# check how many unique vals in 'Security'
data['Security'].nunique()

"""Every value in both 'Ticker Symbol' and 'Security' is unique, so we will drop these columns after the first exploratory data analysis."""

# create countplot for GICS Sector column
sns.countplot(data=data, x='GICS Sector')
plt.xticks(rotation=90)
plt.show()

"""Most datapoints fall into Industrials and Financials GIS Sectors."""

# create countplot for GICS Sub Industry column
plt.figure(figsize=(15, 8))
sns.countplot(data=data, x='GICS Sub Industry')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

"""Data is pretty spread out in terms of sub industries, but most falls into Oil & Gas Exploration & Production, followed by Industrial Conglomerates and REITS."""

#function to plot a box plot and histogram for all numerical variables

def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    f2.suptitle(feature, fontsize=16)  # Add the title to the figure
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    if bins:
        sns.histplot(data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins)
    else:
        sns.histplot(data=data, x=feature, kde=kde, ax=ax_hist2)

    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for title
    plt.show()

# plotting all features

# Loop through each feature in the DataFrame, starting from the 5th column
for feature in data.columns[4:]:
    histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None)

"""1. What does the distribution of stock prices look like?

The prices distribution is right skewed, with the median to the left of the mean. This tells me most datapoints are clustered towards the lower end of the plot, and most of the stock market prices were on the lower end. The long tail also shows me there are a couple of very significant outliers in the price column.

2. The stocks of which economic sector have seen the maximum price increase on average?
"""

# make barplots for price change with GIS Sector on the x axis
plt.figure(figsize=(15, 8))
plt.xticks(rotation=90)
sns.barplot(data=data, x='GICS Sector', y='Price Change')

"""Health care saw the largest price increase on average.

3. How are the different variables correlated with each other?
"""

# create var to hold num vars
num_vars = data.select_dtypes(include=['int64', 'float64'])

sns.heatmap(num_vars.corr(), annot=True)

"""No super high correlations amongst variables, which is good because there may be less issues with multicollinearity. The largest correaltions are positive correlations between Net Income and Estimated Shares Outstanding, Net Income and Earnings Per Share, and Earnings Per Share and Current Price.

4. Cash ratio provides a measure of a company's ability to cover its short-term obligations using only cash and cash equivalents. How does the average cash ratio vary across economic sectors?
"""

# create barplots for cash ratio for each sector
plt.figure(figsize=(15, 8))
plt.xticks(rotation=90)
sns.barplot(data=data, x='GICS Sector', y='Cash Ratio')

"""Information Technology has the highest average cash ratio, but telecommunication services also has a high cash ratio with an even higher range.Utilities has the lowest cash ratio, followed by Industrials.

5. P/E ratios can help determine the relative value of a company's shares as they signify the amount of money an investor is willing to invest in a single share of a company per dollar of its earnings. How does the P/E ratio vary, on average, across economic sectors?
"""

# create barplots for P/E ratio for each sector
plt.figure(figsize=(15, 8))
plt.xticks(rotation=90)
sns.barplot(data=data, x='GICS Sector', y='P/E Ratio')

"""Energy has the highest P/E ratio by far. There are many other sectors that have a relatively low P/E ratio. Telecommunications services is the lowest, followed by financials, industrials, and utilities.

## Data Preprocessing

- Duplicate value check
- Missing value treatment
- Outlier check
- Feature engineering (if needed)
- Any other preprocessing steps (if needed)
"""

#check for duplicates in data
data.duplicated().sum()

"""No duplicates"""

# check for missing values
data.isna().sum()

"""No missing values. No missing value treatment needed."""

#outlier detection using boxplot

plt.figure(figsize=(15, 12))

for i, variable in enumerate(num_vars):
    plt.subplot(4, 4, i + 1)
    plt.boxplot(data[variable], whis=1.5)
    plt.tight_layout()
    plt.title(variable)

plt.show()
plt.savefig("outliers.png")

"""For the most part, the outliers seem to follow a trend, but there are some extremem outliers I want to investigate more to ensure they are real datapoints."""

# check outlier on current price
data[data['Current Price'] >= 1000]

"""Looks legitimate."""

# check outlier on price change
data[data['Price Change'] >= 40]

"""Looks legitimate."""

# check outlier for volatality
data[data['Volatility'] >= 4]

"""Looks legitimate."""

# check outlier for ROE
data[data['ROE'] >= 400]

"""Looks legitimate."""

# check outlier for cash ratio
data[data['Cash Ratio'] >= 600]

"""Looks legitimate."""

# check outliers for net cash flow
data[data['Net Cash Flow'] >= 10000000000]

"""Looks legitimate."""

# check outlier for earnings per share
data[data['Earnings Per Share'] >= 40]

"""Looks legitimate."""

# check outlier for P/E ratio
data[data['P/E Ratio'] >= 200]

"""Looks legitimate."""

# check outlier for P/B ratio
data[data['P/B Ratio'] >= 50]

"""Looks legitimate.

No problematic outliers. I will keep them all, as they all seem to be important data.
"""

data.head()

# feature engineering

# I am going to drop the first 2 columns in the dataset, as they are all unique, so they do not provide us with any useful sorting data.

data.drop(['Ticker Symbol', 'Security'], axis=1, inplace=True)

data.head()

"""We need to scale data before clustering."""

# scaling the data before clustering
scaler = StandardScaler()
data2 = data.drop(['GICS Sector', 'GICS Sub Industry'], axis=1)
data2_scaled = scaler.fit_transform(data2)

# creating a dataframe of the scaled data
data_scaled = pd.DataFrame(data2_scaled, columns=data2.columns)

data_scaled.head()

"""## EDA

- It is a good idea to explore the data once again after manipulating it.
"""

# plotting all features

# Loop through each feature in the DataFrame, starting from the 5th column
for feature in data_scaled:
    histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None)

"""The distribution for stock prices did not change. The prices distribution is right skewed, with the median to the left of the mean. This tells me most datapoints are clustered towards the lower end of the plot, and most of the stock market prices were on the lower end. The long tail also shows me there are a couple of very significant outliers in the price column."""

# create heat map
sns.heatmap(data_scaled.corr(), annot=True)

"""The correlations seem to be unchanged.

## K-means Clustering
"""

# creating a k means dataframe from scaled data

k_means_df = data_scaled.copy()

# Define a range of cluster numbers to evaluate
clusters = range(1, 15)

# Initialize an empty list to store the mean distortions for each k
meanDistortions = []

# Loop over the range of cluster numbers
for k in clusters:
    # Create a KMeans model with k clusters and a fixed random state for reproducibility
    model = KMeans(n_clusters=k, random_state=1)

    # Fit the KMeans model to the subset of the scaled data
    model.fit(data_scaled)

    # Predict the cluster labels for the k_means_df dataset using the fitted model
    prediction = model.predict(k_means_df)

    # Calculate the distortion, which is the average distance bw each point and its nearest cluster center (measure of how tightly clusters are packed)
    distortion = (
        sum(np.min(cdist(k_means_df, model.cluster_centers_, "euclidean"), axis=1))
        / k_means_df.shape[0]
    )

    # Append the calculated distortion to the meanDistortions list
    meanDistortions.append(distortion)

    # Print the number of clusters and the corresponding average distortion
    print("Number of Clusters:", k, "\tAverage Distortion:", distortion)

# Plot the number of clusters (k) against the average distortion
plt.plot(clusters, meanDistortions, "bx-")
plt.xlabel("k")
plt.ylabel("Average Distortion")
plt.title("Selecting k with the Elbow Method")
plt.show()

"""The plot elbow seems to be around 7. However, to be sure, we will use KElbowVisualizer to be sure my assessment of the graph is correct."""

model = KMeans(random_state=1)

# Create a KElbowVisualizer object to help find the optimal number of clusters
# The range of k values to test is from 1 to 15, and timings=True will display the time taken to fit the model for each k
visualizer = KElbowVisualizer(model, k=(1, 15), timings=True)

visualizer.fit(k_means_df)

# Display the Elbow plot, to show relationship bw num of clusters (k) and the avg distortion to identify the optimal k where the elbow occurs
visualizer.show()

"""The plot elbow is actually at 8. So, according to this plot, we would want to create 8 clusters for the best grouping.

Check silhouette scores
"""

# check silhouette scores

sil_score = []
cluster_list = range(2, 15)
for n_clusters in cluster_list:
    clusterer = KMeans(n_clusters=n_clusters, random_state=1)
    preds = clusterer.fit_predict((data_scaled))
    score = silhouette_score(k_means_df, preds)
    sil_score.append(score)
    print("For n_clusters = {}, the silhouette score is {})".format(n_clusters, score))

plt.plot(cluster_list, sil_score)
plt.show()

"""Based on silhouette scores, 3 or 4 seems to be the correct value for k."""

# Finding optimal num of clusters w silhouette coefficients
visualizer = SilhouetteVisualizer(KMeans(3, random_state = 1))
visualizer.fit(data_scaled)
visualizer.show();

# Finding optimal num of clusters w silhouette coefficients
visualizer = SilhouetteVisualizer(KMeans(4, random_state = 1))
visualizer.fit(data_scaled)
visualizer.show();

# Finding optimal num of clusters w silhouette coefficients
visualizer = SilhouetteVisualizer(KMeans(8, random_state = 1))
visualizer.fit(data_scaled)
visualizer.show();

"""3 seems to be the best k value based on silhouette coefficient values."""

# Choosing k=3 clusters
kmeans = KMeans(n_clusters=3, random_state=1)
kmeans.fit(k_means_df)

# creating a copy of the original data
df1 = df.copy()

# adding kmeans cluster labels to the original and scaled dataframes

# add k means segments col to k_means_df
k_means_df['K_means_segments'] = kmeans.labels_

# add k means segments col to df1
df1['K_means_segments'] = kmeans.labels_

# Filter out numeric columns for aggregation
numeric_df = df.select_dtypes(include=[np.number])

# Group by 'K_means_segments' and calculate the mean of numeric columns
cluster_profile = numeric_df.groupby(df['K_means_segments']).mean()

# Count occurrences of values in the 'Security' column for each segment
count_series = df.groupby('K_means_segments')['Security'].count()

# Add the count as a new column in the cluster_profile dataframe
cluster_profile['count_in_each_segments'] = count_series.values

# Display the result
print(cluster_profile)

#visually identify max in each col
cluster_profile.style.highlight_max(color="yellow", axis=0)

# print the companies in each cluster
for cl in df["K_means_segments"].unique():
    print("In cluster {}, the following companies are present:".format(cl))
    print(df[df["K_means_segments"] == cl]["Security"].unique())
    print()

df1.groupby(["K_means_segments", "GICS Sector"])['Security'].count()

plt.figure(figsize=(20, 20))
plt.suptitle("Boxplot of numerical variables for each cluster")

# selecting numerical columns
num_col = df.select_dtypes(include=np.number).columns.tolist()

for i, variable in enumerate(num_col):
    plt.subplot(3, 4, i + 1)
    sns.boxplot(data=df1, x="K_means_segments", y=variable)

plt.tight_layout(pad=2.0)

"""Cluster 0

Has about 32 different securities.
This cluster has securities with a Current_Price of $64 (in between clusters 1 and 2), and negative average Price_Change of -10.56.
As well, cluster has securities with high Volatility of 2.8, ROE of 96.5, P/E Ratio of 111.3, and P/B_Ratio of 1.8.


Cluster 1

Has about 14 different securities.
This cluster has securities with among the lowest average Current_Price of $52.
As well, cluster has securities with highest price change of 6.8, cash ratio of 140.1, Net Cash Flow of 760285714.285714, Net Income of 13368785714.285715, and Estimated Shares Outstanding of 3838879870.871428.


Cluster 2

Has the majority of securities at 294.
This cluster has securities with highest average Current_Price of $84, Earnings Per Share of 3.9, K_means_segments of 2, and count_in_each_segments of 294.


Insights:

-  Cluster 0 represents securities that are currently underperforming in terms of price but may have strong financial metrics, suggesting potential long-term value. However, their high volatility and P/E ratio indicate they are riskier investments.
- Cluster 1 represents securities that are relatively low-priced but have recently seen strong performance and have excellent financial health. These securities may be attractive to investors looking for growth opportunities in large-cap stocks with strong liquidity and profitability.
- Cluster 2 represents the largest group of securities that are relatively higher-priced with strong earnings. This cluster could include well-established companies with stable financial performance, making them attractive to investors looking for stability and consistent returns.

## Hierarchical Clustering
"""

hc_df = data_scaled.copy()

# list of distance metrics
distance_metrics = ["euclidean", "chebyshev", "mahalanobis", "cityblock"]

# list of linkage methods
linkage_methods = ["single", "complete", "average", "weighted"]

high_cophenet_corr = 0
high_dm_lm = [0, 0]

for dm in distance_metrics:
    for lm in linkage_methods:
        Z = linkage(hc_df, metric=dm, method=lm)
        c, coph_dists = cophenet(Z, pdist(hc_df))
        print(
            "Cophenetic correlation for {} distance and {} linkage is {}.".format(
                dm.capitalize(), lm, c
            )
        )
        if high_cophenet_corr < c:
            high_cophenet_corr = c
            high_dm_lm[0] = dm
            high_dm_lm[1] = lm

# printing the combination of distance metric and linkage method with the highest cophenetic correlation
print('*'*100)
print(
    "Highest cophenetic correlation is {}, which is obtained with {} distance and {} linkage.".format(
        high_cophenet_corr, high_dm_lm[0].capitalize(), high_dm_lm[1]
    )
)

#now try diff linkage methods only w eucledian

# list of linkage methods
linkage_methods = ["single", "complete", "average", "centroid", "ward", "weighted"]

high_cophenet_corr = 0
high_dm_lm = [0, 0]

for lm in linkage_methods:
    Z = linkage(hc_df, metric="euclidean", method=lm)
    c, coph_dists = cophenet(Z, pdist(hc_df))
    print("Cophenetic correlation for {} linkage is {}".format(lm, c))
    if high_cophenet_corr < c:
        high_cophenet_corr = c
        high_dm_lm[0] = "euclidean"
        high_dm_lm[1] = lm

# printing the combination of distance metric and linkage method with the highest cophenetic correlation
print(
    "Highest cophenetic correlation is {}, which is obtained with {} linkage".format(
        high_cophenet_corr, high_dm_lm[1]
    )
)

# create dendrogram for the diff linkage methods using eucledian distance

# list of linkage methods
linkage_methods = ["single", "complete", "average", "centroid", "ward", "weighted"]

# lists to save results of cophenetic correlation calculation
compare_cols = ["Linkage", "Cophenetic Coefficient"]

# to create a subplot image
fig, axs = plt.subplots(len(linkage_methods), 1, figsize=(15, 30))

# We will enumerate through the list of linkage methods above
# For each linkage method, we will plot the dendrogram and calculate the cophenetic correlation
for i, method in enumerate(linkage_methods):
    Z = linkage(hc_df, metric="euclidean", method=method)

    dendrogram(Z, ax=axs[i])
    axs[i].set_title(f"Dendrogram ({method.capitalize()} Linkage)")

    coph_corr, coph_dist = cophenet(Z, pdist(hc_df))
    axs[i].annotate(
        f"Cophenetic\nCorrelation\n{coph_corr:0.2f}",
        (0.80, 0.80),
        xycoords="axes fraction",
    )

"""Cophenetic correlation is highest with eucledian distance and average linkages.

4 appears to be the appropriate number of clusters from the dendrogram for average linkage.
"""

# create model

HCmodel = AgglomerativeClustering(n_clusters=6, affinity="euclidean", linkage="average")
HCmodel.fit(hc_df)

# Filter out numeric columns for aggregation
numeric_df = df.select_dtypes(include=[np.number])

# Group by 'K_means_segments' and calculate the mean of numeric columns
cluster_profile = numeric_df.groupby(df['K_means_segments']).mean()

# Count occurrences of values in the 'Security' column for each segment
count_series = df.groupby('K_means_segments')['Security'].count()

# Add the count as a new column in the cluster_profile dataframe
cluster_profile['count_in_each_segments'] = count_series.values

# Display the result
print(cluster_profile)

# Filter out numeric columns for aggregation
numeric_df2 = df.select_dtypes(include=[np.number])

# adding hierarchical cluster labels to the original and scaled dataframes
df_c = df.copy()
df_c["HC_segments"] = HCmodel.labels_
hc_df["HC_segments"] = HCmodel.labels_
numeric_df2["HC_segments"] = HCmodel.labels_
df2["HC_segments"] = HCmodel.labels_

# cluster profiling

hc_cluster_profile = numeric_df2.groupby(df_c["HC_segments"]).mean()

hc_cluster_profile["count_in_each_segments"] = (
    df_c.groupby("HC_segments")["Security"].count().values
)

hc_cluster_profile.style.highlight_max(color="yellow", axis=0)

"""The clusters have securities of 2, 2, 1, 1, and 1. This shows very minimal variability, which is not good."""

# print the companies in each cluster
for cl in df_c["HC_segments"].unique():
    print("In cluster {}, the following companies are present:".format(cl))
    print(df_c[df_c["HC_segments"] == cl]["Security"].unique())
    print()

df_c.groupby(["HC_segments", "GICS Sector"])['Security'].count()

plt.figure(figsize=(20, 20))
plt.suptitle("Boxplot of numerical variables for each cluster")

for i, variable in enumerate(num_col):
    plt.subplot(3, 4, i + 1)
    sns.boxplot(data=df_c, x="HC_segments", y=variable)

plt.tight_layout(pad=2.0)

"""## K-means vs Hierarchical Clustering

1. Which clustering technique took less time for execution?
"""

import time
from sklearn.cluster import KMeans, AgglomerativeClustering

# Timing K-Means with 3 clusters
start_time = time.time()
kmeans = KMeans(n_clusters=3, random_state=1)
kmeans.fit(numeric_df)  # Replace df with your dataset
kmeans_time = time.time() - start_time
print(f"K-Means with 3 clusters execution time: {kmeans_time:.4f} seconds")

# Timing Hierarchical Clustering with 4 clusters
start_time = time.time()
hierarchical = AgglomerativeClustering(n_clusters=4, linkage='average')  # You can change linkage method as needed
hierarchical.fit(numeric_df)  # Replace df with your dataset
hierarchical_time = time.time() - start_time
print(f"Hierarchical Clustering with 4 clusters execution time: {hierarchical_time:.4f} seconds")

# Compare times
if kmeans_time < hierarchical_time:
    print("K-Means with 3 clusters is faster.")
else:
    print("Hierarchical Clustering with 4 clusters is faster.")

"""2. Which clustering technique gave you more distinct clusters, or are they the same?"""

# K-Means clustering with 3 clusters
kmeans_labels = kmeans.labels_
kmeans_silhouette = silhouette_score(numeric_df, kmeans_labels)
print(f"K-Means Silhouette Score: {kmeans_silhouette:.4f}")

# Hierarchical Clustering with 4 clusters
hierarchical_labels = hierarchical.labels_
hierarchical_silhouette = silhouette_score(numeric_df, hierarchical_labels)
print(f"Hierarchical Clustering Silhouette Score: {hierarchical_silhouette:.4f}")

# Compare the silhouette scores
if kmeans_silhouette > hierarchical_silhouette:
    print("K-Means produced more distinct clusters.")
else:
    print("Hierarchical Clustering produced more distinct clusters.")

"""3. How many observations are there in the similar clusters of both algorithms?

Since there were 3 clusters in the k-means model and 4 clusters in the hierarchical model, none of the groups were very similar between models.

4. How many clusters are obtained as the appropriate number of clusters from both algorithms?

The k-means model used only 3 clusters, while the hierarchical model used 4.

## Actionable Insights and Recommendations

- Using the k-means clustering algorithm, Trade&Ahead can input data comprising of stock prices and  financial indicators for companies under the New York Stock Exchange. This will group the data into 3 clusters (since that was the ideal number of clusters for the k-means clustering algorithm), which will help with identifying characteristics of each group, and better inform how to advise their cutomers on which stocks they should invest their money in.
- Using the hierarchical clustering algorithm, Trade&Ahead can input thhe same dataset as the k-means clustering algorithm. However, this model will group the data into 4 clusters instead of 3 (since that was the ideal number of clusters for the hierarchical clustering algorithm), which will also help with identifying characteristics of each group, and better inform how to advise their cutomers on which stocks they should invest their money in.
- The main difference between these 2 algorithms lies in how aggressively you want to separate your data. However, I think providing both these models to the company will allow them to use whichever they think is a better fit, or they can use both, interpret results, then give even more informed advice to their customers.
"""

!jupyter nbconvert PROJECT_stock_Todd-2.ipynb --to html

